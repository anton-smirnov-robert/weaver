<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Definitions on</title><link>https://weaverbird.net/definition/</link><description>Recent content in Definitions on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 07 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://weaverbird.net/definition/index.xml" rel="self" type="application/rss+xml"/><item><title>entropy</title><link>https://weaverbird.net/definition/entropy/</link><pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate><guid>https://weaverbird.net/definition/entropy/</guid><description>For a given phase space, Hamiltonian and energy, entropy quantifies the number of micro-states available.
Details In the Gibbs (canonical) ensemble (NVT), the partition function reads $$ Z = \sum_i e^{-\beta E_i} $$an with the probability of the micro-state $i$ being $p_i=e^{-\beta E_i} / Z$, we have $$ S_G=-k_B \sum_i p_i \log(p_i) $$</description></item><item><title>negentropy</title><link>https://weaverbird.net/definition/negentropy/</link><pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate><guid>https://weaverbird.net/definition/negentropy/</guid><description>First mentionned in Schrodinger1944 to characterize living systems, it is rigourously introduced by Brillouin1951 to relate it to the opposite of entropy and positively to information (see Brillouin&amp;rsquo;s relation between entropy and information).</description></item></channel></rss>