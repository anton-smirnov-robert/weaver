{"/":{"title":"Weaver","content":"","lastmodified":"2022-12-07T21:46:47.989730037Z","tags":null},"/Where-am-I":{"title":"Where am I?","content":"\nWelcome!\n\nYou are on my personal website. You can find out [about me](about%20me.md) by clicking on the link.\n\nIn fact, this entire website relies on forward and backward links (backlinks) between pages as you can see on the local graph just below. \nAt any moment you can go back to the global graph containing all pages by cliking on **complexus** at the top of the page or **Home** at the bottom.\n\nIf you are here for the first time, let me introduce you to the project in four steps.r\n\n\u003e[!warning]  \n\u003e\n\u003eCan i say something here ? \n\n\n\n\n[list of concepts](/tags/concept)\n","lastmodified":"2022-12-07T21:46:47.989730037Z","tags":null},"/about-me":{"title":"about me","content":"\nI'm Anton Robert.\n\n[manifesto](manifesto.md)\n","lastmodified":"2022-12-07T21:46:47.989730037Z","tags":null},"/concept/Brillouins-relation-between-entropy-and-information":{"title":"Brillouin's relation between entropy and information","content":"\nIn 1951, Brilliouin related information and entropy to resolve [Maxwell's demon paradox](concept/Maxwell's%20demon%20paradox.md); more precisely [Shannon's theory of information transmission](concept/Shannon's%20theory%20of%20information%20transmission.md) and the [second principe of thermodynamics](concept/second%20principe%20of%20thermodynamics.md). He used the word  [negentropy](concept/negentropy.md) (first used in [Schrodinger1944](reference/Schrodinger1944.md)). Here, it reads **high entropy = low information**. \n\n#### Sources\n\n[Brillouin1951](reference/Brillouin1951.md)     \n[Brillouin1956](reference/Brillouin1956.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Cantors-diagonal-argument":{"title":"Cantor's diagonal argument","content":"\nOne of Cantor's theorem is that the set of all sets of natural numbers is not enumerable. \nThe set of real numbers is the set of all set of natural numbers because you form a real number by precising its decimals. It is therefore not an [enumerable set](definition/enumerable%20set.md).\n\n#### Proof: \nWe consider the set of all sets of natural numbers $SS=\\{S_1,S_2,...\\}$ and there characteristic functions $s_1,s_2, ...$ . $s_n(m)=1 \\ if \\  m\\in S_n$  and $0$ if not. We can construct a matrix with $M_{nm}=s_n(m)$. We want to show that there is a set that is not in $SS$. \n\nConsider the *antidiagonal* sequence  $\\{1-s_n(n)\\}_n$. If this set is already in $SS$ it means that $\\exists n , \\forall m , s_n(m)=1-s_m(m)$ . For $m=n$  it would mean that $s_n(n)=1-s_n(n)$. Such $n$  does not exist and therefore, this set is missing from $SS$: is does not contain all sets of natural numbers.\n \n\n#### Sources \n\n[Boolos2007](reference/Boolos2007.md)\n\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Church-Turing-thesis":{"title":"Church-Turing thesis","content":"\n*An [arithmetic function](definition/arithmetic%20function.md) is [effectively computable](definition/effectively%20computable.md) if and only if it is computable by a [[concept/Turing machine]] (or — using the [computable](definition/computable.md) equivalence — if it is is a [recursive function](definition/recursive%20function.md))*.\n\nThis is a thesis because it is not provable because it refers to the informal [effectively computable](definition/effectively%20computable.md) notion. We know for sure one way (the return) because a Turing machine is computation is a type of effective computation.\n\nWhy believe it? All attempts to give an exact analysis of the intuitive notion of an effectively calculable function have turned out to be _equivalent_, in the sense that each analysis offered has been proved to pick out the same class of functions, namely those that are computable by Turing machine.\nChurch has made such an hypothesis independently with other sort of computation (abacus machine).\n\n#### Sources \n[Standford Encyclopedia](https://plato.stanford.edu/entries/church-turing/#MisuThes)\n[Wikipedia](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis)\n\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Cricks-central-dogma":{"title":"Crick's central dogma","content":"\nIt postulates that proteins never transmit \"sequential information\".\n\n\"Sequential information\" is to be understood as the sequence of bases in DNA or amino-acids in proteins. \n\n#### Sources\n\n[Crick1958](reference/Crick1958.md)\n[Crick1970](reference/Crick1970.md)\n[Wikipedia](https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology#:~:text=The%20central%20dogma%20of%20molecular,The%20Central%20Dogma.)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Maxwells-demon-paradox":{"title":"Maxwell's demon paradox","content":"\nIt was suggested in 1867 by Maxwell. A 'finite being' can open or close a door between two boxes. Depending on the velocity of the particle that is coming close to the door, he lets fast particles go to one side and slow ones on the other side. Therefore, he cools down one box and heats up the other one and \"violates\" [second principe of thermodynamics](concept/second%20principe%20of%20thermodynamics.md). This paradox was tackled by [Brillouin's relation between entropy and information](concept/Brillouin's%20relation%20between%20entropy%20and%20information.md). ","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Mendelian-inheritance":{"title":"Mendelian inheritance","content":"\nIn 1865, the monk and botanist Gregor Mendel find that a trait can different forms (alleles) and suggested three laws:\n\n- Some alleles are dominant while others are recessive.\n- Each gamete carries only one allele for each gene.\n- Genes of different traits can segregate independently during the formation of gametes (meiose).\n\n#### Sources \n\n[Wikipedia](https://en.wikipedia.org/wiki/Mendelian_inheritance)\n\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Schr%C3%B6dingers-code-script-aperiodic-crystal":{"title":"Schrödinger's code-script aperiodic crystal","content":"\nIn 1944, Schrödinger imagine that a “code-script” could be found in “aperiodic crystals” (chromosomes) of living systems ([Schrodinger1944](reference/Schrodinger1944.md)). Without mentionning “information”, it does refer to a discrete encoding and therefore implies Laplacian determinism (and predictability). This was also acknowledge by the author. \n\n\n\u003e[!quote]\n\u003e\n\u003e*In calling the structure of the chromosome fibers a code-script we mean that the all-penetrating mind, once conceived by Laplace, (...) could tell from their structure whether the egg would develop, under suitable conditions, into a black cock or into a speckled hen* — (p. ?)","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Shannons-theory-of-information-transmission":{"title":"Shannon's theory of information transmission","content":"Be $X$  a random variable with $X\\in\\{x_1,x_2,...,x_n\\}$ and $p_i=\\mathbb{P}(X=x_i)$. Shannon relates the amount of information of one string to the amount of surprise of seeing it. The information is defined as $$I(X) = - \\log(P(X))$$and the expected value gives the amount of  \"**information, choice or uncertainty**\" — [Shannon1948](reference/Shannon1948.md) as follows  $$H(X)=E(I(X))=-\\sum_i p_i \\log(p_i)$$This formula happens to be the same as the one for Gibbs [entropy](definition/entropy.md) (with an additional Boltzmann constant) in the framework of the [second principe of thermodynamics](concept/second%20principe%20of%20thermodynamics.md). Therefore, Shannon makes the link between information and positive entropy (see [Brillouin's relation between entropy and information](concept/Brillouin's%20relation%20between%20entropy%20and%20information.md)). This is peculiar because as entropy (and disorder) increases, information is lost. It is now preferred to give the name **entropy** to $H(X)$.  We use the meaning of **uncertainty** in this case. \n\n#### Exemple(s)\nA series of fair coin tosses have a maximal entropy $H$ since all throw are equiprobable (maximal uncertainty). \n\n#### Sources \n[Shannon1948](reference/Shannon1948.md)\nnote [on Longo2012](note/on%20Longo2012.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/Turing-machine":{"title":"Turing machine","content":"In 1936, Turing introduces the idea of a “computing machine”, latter called “Turing machine”. He pictures not a mecanism but a person — “Turing's architect” according to ([Schrodinger1944](reference/Schrodinger1944.md)) Turing introduced ([Turing1936](reference/Turing1936.md)) this machine trying to answer questions regarding decidability. A Turing machine is an [automaton](definition/automaton.md) with an unlimited and unrestricted memory. It is a model of everything that can do today's computers. It requires a (readable, writtable, and movable) infinite 1D bit register — a tape. Turing has shown that those elementary instructions are enough to do elaborate any sophisticated program that are themselves encoded on the tape. The Turing machine allocates a data space to make the computation and another one where the programs are encoded. The operating system read the instructions as it computes the program and change the input binary string. Increasing the complexity of the machine (e.g. alphabet, dimensions) does not change the class of functions it can computes ([recursive function](definition/recursive%20function.md)s). \n\n\n#### Details \n\n**What is the formal definition of a Turing machine ?** \nThe alphabet of the tape $\\Gamma$ can be thought to be 0's and 1's and blank's. \n\n$$\\begin{array}{l}\\text { A Turing machine is a 7-tuple, }\\left(Q, \\Sigma, \\Gamma, \\delta, q_{0}, q_{\\text {accept }}, q_{\\text {reject }}\\right) \\text {, where } \\\\ Q, \\Sigma, \\Gamma \\text { are all finite sets and } \\\\ \\text { 1. } Q \\text { is the set of states, } \\\\ \\text { 2. } \\Sigma \\text { is the input alphabet not containing the blank symbol } \\sqcup \\text {, } \\\\ \\text { 3. } \\Gamma \\text { is the tape alphabet, where } \\sqcup \\in \\Gamma \\text { and } \\Sigma \\subseteq \\Gamma \\text {, } \\\\ \\text { 4. } \\delta: Q \\times \\Gamma \\longrightarrow Q \\times \\Gamma \\times\\{\\mathrm{L}, \\mathrm{R}\\} \\text { is the transition function, } \\\\ \\text { 5. } q_{0} \\in Q \\text { is the start state, } \\\\ \\text { 6. } q_{\\text {accept }} \\in Q \\text { is the accept state, and } \\\\ \\text { 7. } q_{\\text {reject }} \\in Q \\text { is the reject state, where } q_{\\text {reject }} \\neq q_{\\text {accept }}\\end{array}$$\n\n**How to write the program (or diagram or table of instructions) in the machine ?**\n\nWe write $S_0=blank$ , $S_1=0$ and $S_2=1$, $R=move \\ right$, $L= move \\ left$ . \nAn instruction can be written $q_i S_i S_j R q_m$  or  $q_i S_i S_j L q_m$. We encode all instructions that define the Turing machine, as follows $$q_1 S_0 S_0 L q_2 ; q_2 S_1 S_2 L q_3 ; ... $$We write $q_i=DAAAA...A$ ($i$  times), $S_i=DCCC...C$ ($i$  times) and give a integer to each letter (A=1, C=2, D=3,...,;=7). Therefore, it gives a description number for the Turing machine $$313325117 ...$$There is a unique integer for a given machine, but we can always add unecessary programs (that change the number but that give the same result). This integer gets a binary decomposition$$01000101011010$$With the help of an additional small and instantaneous \"workspace\", it uses a [simple mecanism](https://www.youtube.com/watch?v=P66h8D5Lkwk) to read the programs and compute at the same time.\n\n\n#### Sources \n[Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/turing-machine/#TuriDefi)   \n[MIT Open courses](https://ocw.mit.edu/courses/18-404j-theory-of-computation-fall-2020/pages/syllabus/)     \n[Sipser2013](reference/Sipser2013.md)\nnote [on Longo2012](note/on%20Longo2012.md)     \n[Youtube](https://www.youtube.com/watch?v=P66h8D5Lkwk)         \n[Wikipedia](https://en.wikipedia.org/wiki/Turing_machine)       \n[Turing1936](reference/Turing1936.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/halting-problem":{"title":"halting problem","content":"\nGiven a [[concept/Turing machine]] and an initial binary strings, the problem consists in knowing if it will halt or not. This problem cannot be solve on a [[concept/Turing machine]] because the halting function is [[definition/uncomputable]]. \n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/negentropy":{"title":"negentropy","content":"\nFirst mentionned in [Schrodinger1944](reference/Schrodinger1944.md) to characterize living systems, it is rigourously introduced by [Brillouin1951](reference/Brillouin1951.md) to relate it to the opposite of [entropy](definition/entropy.md) and positively to information (see [Brillouin's relation between entropy and information](concept/Brillouin's%20relation%20between%20entropy%20and%20information.md)). Providing the ad-hoc homogeneity, it reads $$N=-S= I$$\n#### Sources \n\n[Brillouin1956](reference/Brillouin1956.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/concept/second-principe-of-thermodynamics":{"title":"second principe of thermodynamic","content":"\nThe [entropy](definition/entropy.md) of a **closed system** with **fixed volume** can only increase. \n\nIt is maximum at equilibrium for a system at constant energy. In other words, the system evolves (if not already equilibrated) towards the macro-state that is invariant under the most number of microstates. \n\nThe first law of thermodynamic is the conservation of energy (electric, kinetic, mechanical, chemical,...). The second says that, without external aid, there is a \"dispersion of energy\" such that it tends to make the distribution of energy homogeneous. \n\nOriginally, this is about thermodynamical cycles (Carnot) and “aid” (for machines):\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/abacus":{"title":"abacus","content":"\nAn abacus is a \"higher-level\" (more flexible) [Turing machine](concept/Turing%20machine.md) for computation.\nEvery abacus-computable function is Turing computable. \nAll [recursive function](definition/recursive%20function.md)s are abacus-computable. \n\n#### Source(s)\n\n[Boolos2007](reference/Boolos2007.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/arithmetic-function":{"title":"arithmetic function","content":"\nA function whose domain is $\\mathbb{N}$.","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/automaton":{"title":"automaton","content":"\nFormally, a finite automaton can be defined as \n$$\\begin{array}{l}\\text { A finite automaton (or machine) is a } 5 \\text {-tuple } M=\\left(Q, \\Sigma, \\delta, q_{0}, F\\right) \\text {, where } \\\\ \\text { 1. } Q \\text { is a finite set called the states, } \\\\ \\text { 2. } \\Sigma \\text { is a finite set called the alphabet, } \\\\ \\text { 3. } \\delta: Q \\times \\Sigma \\longrightarrow Q \\text { is the transition function, } \\\\ \\text { 4. } q_{0} \\in Q \\text { is the start state, and } \\\\ \\text { 5. } F \\subseteq Q \\text { is the set of accept states. }{ }\\end{array}$$\nIt takes as input an binary *string* to gives the states $q_i$ . Some input strings are accepted (because you reach the final state or \"accepted state\"), some are rejected. All the input strings form the *language* of the machine $L(M)$. \n\n\n#### Sources \n[Sipser2013](reference/Sipser2013.md)    \n[MIT Open cour. es](https://ocw.mit.edu/courses/18-404j-theory-of-computation-fall-2020/pages/syllabus/)","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/computable":{"title":"computable","content":"\nThree notions of computability are equivalent: \n- The existence of a [Turing machine](concept/Turing%20machine.md) computing the function.\n- The [abacus](definition/abacus.md)-computability.\n- A [recursive function](definition/recursive%20function.md)\n\nA function is computable on a [Turing machine](concept/Turing%20machine.md) if and only if it is a [recursive function](definition/recursive%20function.md)\n\n[Church-Turing thesis](concept/Church-Turing%20thesis.md) assumes that it is the same thing as [effectively computable](definition/effectively%20computable.md).\nSome functions are however [uncomputable](definition/uncomputable.md).\n\n\n#### Sources \n\n[Boolos2007](reference/Boolos2007.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/effectively-computable":{"title":"effectively computable","content":"\nThis *informal notion* also refers to: \n- procedure \n- paper-and-pencil method or \n- to a mechanical method \n\nIt is specific to a type of problem but we can state:\n\n- A finite number of instructions.\n- It always terminates after a finite number of steps.\n- It always produces a correct answer.\n- It can be done by a human without any aids except writing materials.\n- Its instructions need only to be followed rigorously to succeed : it requires no ingenuity\n\n#### Source(s)\n\n[Wikipedia](https://en.wikipedia.org/wiki/Effective_method)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/entropy":{"title":"entropy","content":"\nFor a given phase space, Hamiltonian and energy, entropy quantifies the number of micro-states available.\n\n#### Details \n\nIn the Gibbs (canonical) ensemble (NVT), the partition function reads $$ Z = \\sum_i e^{-\\beta E_i} $$an with the probability of the micro-state $i$  being  $p_i=e^{-\\beta E_i} / Z$, we have \n$$ S_G=-k_B \\sum_i p_i \\log(p_i) $$\n\nFor a fixed energy, in the micro-canonical ensemble (NVE), the energy is fixed and we get $$ \\Omega = \\sum_i 1 $$ and Boltzmann's entropy  $$ S_B = k_B \\log(\\Omega )$$They are equal by restricting the sum for a given energy $E$ in the summation of the (NVT) ensemble. It gives $Z=e^{-\\beta E}\\Omega$ and $S_G = S_B$.\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/enumerable-set":{"title":"enumerable set","content":"\nA set is enumerable if and only if it is the range of some [arithmetic function](definition/arithmetic%20function.md).\n\n(The range is the set of the function's values.\nThe domain is the set of the functions's argument.)\n\n#### Source(s)\n\n[Boolos2007](reference/Boolos2007.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/recursive-function":{"title":"recursive function","content":"\nIt refers to functions that can be built with the zero, the successor and the identity function by use of composition, minimization and primitive recursion: $$h(x,0)=f(x), \\ h(x,y')=g(x,y,h(x,y))$$\nThey are all [effectively computable](definition/effectively%20computable.md). \n\n#### Exemple(s) \n\n#### Sources \n\n[Boolos2007](reference/Boolos2007.md)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/definition/uncomputable":{"title":"uncomputable","content":"\nDue to [Cantor's diagonal argument](concept/Cantor's%20diagonal%20argument.md), we know that the set of [arithmetic function](definition/arithmetic%20function.md)s that have a range on positive integers is not an [enumerable set](definition/enumerable%20set.md). But the set of [Turing machine](concept/Turing%20machine.md)s is an [enumerable set](definition/enumerable%20set.md) because they can be expressed as quadruplets. There are functions are not computable by a [Turing machine](concept/Turing%20machine.md). Assuming [Church-Turing thesis](concept/Church-Turing%20thesis.md), they are functions that are not [effectively computable](definition/effectively%20computable.md). For instance, solving the [halting problem](concept/halting%20problem.md) requires the introduction of the halting function (that takes as argument a Turing machine and a binary string) that is not Turing-computable. \n\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/manifesto":{"title":"manifesto","content":"\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/note/on-Longo2012":{"title":"on Longo2012","content":"\n## **Is Information a proper observable for biological organization?**     \nby Longo G., Miquel P., Sonnenschein C., Soto A.M. (2012)   \n*Progress in biophysics and molecular biology* 109: 108-114      \nDOI: 10.1016/j.pbiomolbio.2012.06.004\n\nReference: [Longo2012](reference/Longo2012.md)\n\n\n#### DNA was discovered in times that proned an informational metaphor to describe biological organization.\n\nThe discovery of DNA by Crick and Watson in 1953 happened just after an flourishing era of mathematical logic and computation. The concepts of (hardware and software-independent) [computable](definition/computable.md), elaboration of information (reading and programming) with [Turing machine](concept/Turing%20machine.md)s were developped in 1930-1936. At the end of WW2 (were cryptography played a major role), [Schrödinger's code-script aperiodic crystal](concept/Schrödinger's%20code-script%20aperiodic%20crystal.md) hints at informational encoding in chromosomes, such as *programs*. In 1948, [Shannon's theory of information transmission](concept/Shannon's%20theory%20of%20information%20transmission.md), which is still crucial nowadays also poped up. The informational metaphor for biological organization (without the hypothesis of the abstract model of mathematical logic) laid out the groundwork for a simplistic explanation of [Mendelian inheritance](concept/Mendelian%20inheritance.md) with the four bases discovered in the DNA. This metaphor is at the core of [Crick's central dogma](concept/Crick's%20central%20dogma.md). \n\n\n\n\n\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Boolos2007":{"title":"Boolos2007","content":"\n**Computability and Logic**     \nby Boolos G.S., Burgess J.P., Jeffrey R.C. (2007)\nCambridge University Press\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Brillouin1951":{"title":"Brillouin1951","content":"\n**Maxwell's Demon Cannot Operate: Information and Entropy. I**     \nby Brillouin L. (1951)\n*Journal of Applied Physics* 22: 334-337    \nDOI: 10.1063/1.1699951\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Brillouin1956":{"title":"Brillouin1956","content":"\n**Science and information theory**     \nby Brillouin L. (1956)\nAcademic Press\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Crick1958":{"title":"Crick1958","content":"\n**On protein synthesis**     \nby Crick F.H. (1958)\n*Symp Soc Exp Biol* 12: 8    \n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Crick1970":{"title":"Crick1970","content":"\n**Central Dogma of Molecular Biology**     \nby Crick F. (1970)\n*Nature* 227: 561-563    \nDOI: 10.1038/227561a0\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Longo2012":{"title":"Longo2012","content":"\n**Is Information a proper observable for biological organization?**     \nby Longo G., Miquel P., Sonnenschein C., Soto A.M. (2012)\n*Progress in biophysics and molecular biology* 109: 108-114    \nDOI: 10.1016/j.pbiomolbio.2012.06.004\n\n\n\n\u003e [!quote] \n\u003e\n\u003e*So, the “constraints” or the interactions in living organisms, given by either the cells, the different levels of organization and/or by the ecosystems cannot causally and positively contribute to the generation and maintenance of the organism unless they are digitally encoded as molecular signs.* —  (Longo et al., 2012, p. 6)  \n \n\n\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Schrodinger1944":{"title":"Schrodinger1944","content":"\n**What is Life?: With Mind and Matter and Autobiographical Sketches**     \nby Schrödinger E. (1944)\nCambridge University Press\n\n\n\n\u003e[!quote]\n\u003e\n\u003e*In calling the structure of the chromosome fibers a code-script we mean that the all-penetrating mind, once conceived by Laplace, (...) could tell from their structure whether the egg would develop, under suitable conditions, into a black cock or into a speckled hen* — (p. ?)\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Shannon1948":{"title":"Shannon1948","content":"\n**A mathematical theory of communication**     \nby Shannon C.E. (1948)\n*The Bell system technical journal* 27: 379–423    \n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Sipser2013":{"title":"Sipser2013","content":"\n**Introduction to the theory of computation**     \nby Sipser M. (2013)\nCengage Learning\n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null},"/reference/Turing1936":{"title":"Turing1936","content":"\n**On computable numbers, with an application to the Entscheidungsproblem**     \nby Turing A.M. (1936)\n*J. of Math* 58: 5    \n","lastmodified":"2022-12-07T21:46:47.993730014Z","tags":null}}